{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768bb55c",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "\n",
    "목표: $\\frac{\\partial L}{\\partial w}$ 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498c880",
   "metadata": {},
   "source": [
    "## Numerical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c3dc3",
   "metadata": {},
   "source": [
    "### Simple numerical gradient with independent variable w\n",
    "\n",
    "$w$ 값을 $\\pm h$한 상태로 $f(w)$를 계산함 (= $\\frac{\\partial f}{\\partial w}$)\n",
    "\n",
    "$w \\rightarrow f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b5c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def numerical_gradient(f, w):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    for i in range(w.size):\n",
    "        tmp = w[i]\n",
    "        w[i] = tmp+h\n",
    "        f1 = f(w)\n",
    "        \n",
    "        w[i] = tmp-h\n",
    "        f2 = f(w)\n",
    "        \n",
    "        grad[i] = (f1-f2)/(2*h)\n",
    "        w[i] = tmp\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8732b",
   "metadata": {},
   "source": [
    "$f(w_0, w_1) = w_0^2 + w_1^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b72f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w):\n",
    "    return w[0]**2 + w[1]**2\n",
    "\n",
    "numerical_gradient(f, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d4805b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(f, w, lr=0.1, step_num=100):    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, w)\n",
    "        w -= lr * grad\n",
    "        \n",
    "    return w\n",
    "\n",
    "init_w = np.array([-3.0, 4.0])\n",
    "gradient_descent(f, init_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d7fa9",
   "metadata": {},
   "source": [
    "### Numerical gradient on neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e71a63",
   "metadata": {},
   "source": [
    "$W$ 값을 $\\pm h$한 상태로 $loss(x, t)$를 계산함 (= $\\frac{\\partial L}{\\partial W}$ = `dW`) ⭐\n",
    "\n",
    "$W, x \\rightarrow z$\n",
    "\n",
    "$z \\rightarrow y$\n",
    "\n",
    "$y, t \\rightarrow loss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007e256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/functions.py\n",
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/gradient.py\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f0d055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48630894 -1.11892498  0.70457065]\n",
      " [-0.66926975 -0.71634148  0.67094492]]\n",
      "0.21718451994138732\n",
      "[[ 0.0707409   0.04639027 -0.11713117]\n",
      " [ 0.10611135  0.0695854  -0.17569676]]\n"
     ]
    }
   ],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "print(net.loss(x, t))\n",
    "\n",
    "\n",
    "def f(W): # ⭐\n",
    "    return net.loss(x, t) # ⭐\n",
    "\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9471535",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a1ba8",
   "metadata": {},
   "source": [
    "### Addition\n",
    "\n",
    "$z = x + y$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "$x, y \\rightarrow z$\n",
    "\n",
    "Backward propagation:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z} \\cdot 1, \\frac{\\partial L}{\\partial z} \\cdot 1 \\leftarrow \\frac{\\partial L}{\\partial z}$\n",
    "\n",
    "덧셈노드의 역전파는 상류의 값을 그대로 하류로 흘려보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e19f11",
   "metadata": {},
   "source": [
    "### Multiplication\n",
    "\n",
    "$z = xy$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "$x, y \\rightarrow z$\n",
    "\n",
    "Backward propagation:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z} \\cdot y, \\frac{\\partial L}{\\partial z} \\cdot x \\leftarrow \\frac{\\partial L}{\\partial z}$\n",
    "\n",
    "곱셈노드의 역전파는 순전파 때의 값을 서로 바꿔 곱해 하류로 흘려보냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b59d9",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "        x & (x>0)\\\\\n",
    "        0 & (x\\leq0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\begin{cases}\n",
    "        1 & (x>0)\\\\\n",
    "        0 & (x\\leq0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    x \\rightarrow y=x & (x>0)\\\\\n",
    "    x \\rightarrow y=0 & (x\\leq0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Backward propagation:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\frac{\\partial L}{\\partial y} \\leftarrow \\frac{\\partial L}{\\partial y} & (x>0)\\\\\n",
    "    0 \\leftarrow \\frac{\\partial L}{\\partial y} & (x\\leq0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400049f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        # 0보다 작으면 mask = True\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04608935",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "$y = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "$x \\rightarrow -x \\rightarrow \\exp(-x) \\rightarrow 1+\\exp(-x) \\rightarrow \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "`/`: $y = \\frac{1}{x}$, $\\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2$\n",
    "\n",
    "`+`: 상류의 값을 그대로 하류로 흘려보냄\n",
    "\n",
    "`exp`: $y = \\exp(x)$, $\\frac{\\partial y}{\\partial x} = \\exp(x)$\n",
    "\n",
    "`*`: 순전파 때의 값을 서로 바꿔 곱해 하류로 흘려보냄\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} y^2 \\exp(-x) \\leftarrow -\\frac{\\partial L}{\\partial y} y^2 \\exp(-x) \\leftarrow -\\frac{\\partial L}{\\partial y} y^2 \\leftarrow -\\frac{\\partial L}{\\partial y} y^2 \\leftarrow \\frac{\\partial L}{\\partial y}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} y^2 \\exp(-x) = \\frac{\\partial L}{\\partial y}\\frac{1}{(1+\\exp(-x))^2}\\exp(-x) = \\frac{\\partial L}{\\partial y}\\frac{1}{1+\\exp(-x)}\\frac{\\exp(-x)}{1 + \\exp(-x)} = \\frac{\\partial L}{\\partial y}y(1-y)$\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "$x \\rightarrow y$\n",
    "\n",
    "Backward propagation:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} y (1-y) \\leftarrow \\frac{\\partial L}{\\partial y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41846d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642868c5",
   "metadata": {},
   "source": [
    "### Affine\n",
    "\n",
    "$X \\cdot W + B = Y$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$, $\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$, $\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$ ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ae3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) # ⭐\n",
    "        self.dW = np.dot(self.x.T, dout) # ⭐\n",
    "        self.db = np.sum(dout, axis=0) # ⭐\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16985017",
   "metadata": {},
   "source": [
    "### Softmax-with-Loss\n",
    "\n",
    "Softmax의 손실 함수로 Cross Entropy Error를 사용하면 역전파가 $(y_1-t, y_2-t, y_3-t)$로 말끔히 떨어짐\n",
    "\n",
    "$L = -(t \\log(y) + (1-t) \\log(1-y))$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} = - \\frac{t}{y} + \\frac{1-t}{1-y}$\n",
    "\n",
    "$y = \\frac{1}{1+\\exp(-x)}$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = y(1-y)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = \\left( - \\frac{t}{y} + \\frac{1-t}{1-y} \\right) \\cdot y(1-y) = y-t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b824d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = np.exp(x) / np.sum(np.exp(x))\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213943d1",
   "metadata": {},
   "source": [
    "### Complete neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a342ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/layers.py\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23202efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51068f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/dataset/mnist.py\n",
    "\n",
    "    \n",
    "# coding: utf-8\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.getcwd() # dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_ont_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNIST 데이터셋 읽기\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    one_hot_label : \n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_ont_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_ont_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192c8292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.422962173310787e-10\n",
      "b1:3.3495286000530906e-09\n",
      "W2:6.914304188297227e-09\n",
      "b2:1.4066889665459436e-07\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6346b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1131 0.1176\n",
      "0.9052 0.9060\n",
      "0.9234 0.9252\n",
      "0.9359 0.9382\n",
      "0.9458 0.9465\n",
      "0.9503 0.9481\n",
      "0.9547 0.9528\n",
      "0.9601 0.9571\n",
      "0.9642 0.9590\n",
      "0.9655 0.9612\n",
      "0.9683 0.9634\n",
      "0.9698 0.9630\n",
      "0.9729 0.9660\n",
      "0.9732 0.9668\n",
      "0.9755 0.9668\n",
      "0.9758 0.9686\n",
      "0.9767 0.9689\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f'{train_acc:.4f} {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9578ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
