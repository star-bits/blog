{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- [YouTube lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ],
      "metadata": {
        "id": "YqCUiYGthGiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOmqCjopiKux"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "\n",
        "n_embed = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout_ratio = 0.0\n",
        "\n",
        "max_iters = 5000\n",
        "learning_rate = 1e-3\n",
        "eval_interval = 100\n",
        "eval_iters = 200 # for calculating mean loss\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmFNHU_XALo1",
        "outputId": "fcd7c060-1c9a-4277-f7a7-99ba9c11bdd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x789c581e0ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # random index starting point for x\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc0R7FMQBKVx",
        "outputId": "42e1fe79-d70d-4722-80f3-5283c31de4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-15 11:38:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-10-15 11:38:17 (154 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.W_k = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.W_q = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.W_v = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # ?\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.W_k(x) # (B, T, C)\n",
        "        q = self.W_q(x) # (B, T, C)\n",
        "\n",
        "        att = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
        "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        att = F.softmax(att, dim=-1) # (B, T, T)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        v = self.W_v(x) # (B, T, C)\n",
        "        out = att @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4*n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embed, n_embed),\n",
        "            nn.Dropout(dropout_ratio),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.mha = MultiHeadAttention(n_head, head_size)\n",
        "        self.ff = FeedFoward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size) # lm stands for language model\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None): # indices as input\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.blocks(x) # (B, T, C)\n",
        "        x = self.ln_f(x) # (B, T, C)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:] # because T dim was lengthened by previous generation\n",
        "\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "T3XFieK0M_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr94j_xkBgso",
        "outputId": "dcd3f416-6b1d-4701-c0d7-aa9e6ce91ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsT9JBsWQhvO",
        "outputId": "e4d6d62a-20f1-4a54-a556-e6f0a5c40b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1959, val loss 4.1962\n",
            "step 100: train loss 2.6273, val loss 2.6203\n",
            "step 200: train loss 2.4603, val loss 2.4549\n",
            "step 300: train loss 2.3937, val loss 2.4065\n",
            "step 400: train loss 2.3327, val loss 2.3335\n",
            "step 500: train loss 2.2658, val loss 2.2847\n",
            "step 600: train loss 2.2115, val loss 2.2437\n",
            "step 700: train loss 2.1539, val loss 2.1753\n",
            "step 800: train loss 2.1181, val loss 2.1513\n",
            "step 900: train loss 2.0714, val loss 2.1182\n",
            "step 1000: train loss 2.0651, val loss 2.1102\n",
            "step 1100: train loss 2.0231, val loss 2.0826\n",
            "step 1200: train loss 1.9973, val loss 2.0805\n",
            "step 1300: train loss 1.9875, val loss 2.0530\n",
            "step 1400: train loss 1.9551, val loss 2.0490\n",
            "step 1500: train loss 1.9138, val loss 2.0076\n",
            "step 1600: train loss 1.9003, val loss 2.0111\n",
            "step 1700: train loss 1.8913, val loss 1.9996\n",
            "step 1800: train loss 1.8875, val loss 1.9881\n",
            "step 1900: train loss 1.8575, val loss 1.9762\n",
            "step 2000: train loss 1.8413, val loss 1.9621\n",
            "step 2100: train loss 1.8382, val loss 1.9826\n",
            "step 2200: train loss 1.8167, val loss 1.9462\n",
            "step 2300: train loss 1.8114, val loss 1.9536\n",
            "step 2400: train loss 1.7918, val loss 1.9372\n",
            "step 2500: train loss 1.7961, val loss 1.9399\n",
            "step 2600: train loss 1.7649, val loss 1.9104\n",
            "step 2700: train loss 1.7641, val loss 1.9158\n",
            "step 2800: train loss 1.7554, val loss 1.9031\n",
            "step 2900: train loss 1.7507, val loss 1.9018\n",
            "step 3000: train loss 1.7592, val loss 1.9108\n",
            "step 3100: train loss 1.7471, val loss 1.9130\n",
            "step 3200: train loss 1.7362, val loss 1.8987\n",
            "step 3300: train loss 1.7194, val loss 1.8805\n",
            "step 3400: train loss 1.7161, val loss 1.8816\n",
            "step 3500: train loss 1.7165, val loss 1.8761\n",
            "step 3600: train loss 1.7159, val loss 1.8738\n",
            "step 3700: train loss 1.7052, val loss 1.8703\n",
            "step 3800: train loss 1.7038, val loss 1.8676\n",
            "step 3900: train loss 1.6870, val loss 1.8434\n",
            "step 4000: train loss 1.6839, val loss 1.8392\n",
            "step 4100: train loss 1.6778, val loss 1.8571\n",
            "step 4200: train loss 1.6739, val loss 1.8564\n",
            "step 4300: train loss 1.6784, val loss 1.8462\n",
            "step 4400: train loss 1.6710, val loss 1.8501\n",
            "step 4500: train loss 1.6721, val loss 1.8423\n",
            "step 4600: train loss 1.6508, val loss 1.8456\n",
            "step 4700: train loss 1.6591, val loss 1.8239\n",
            "step 4800: train loss 1.6595, val loss 1.8491\n",
            "step 4900: train loss 1.6414, val loss 1.8305\n",
            "step 4999: train loss 1.6571, val loss 1.8526\n",
            "\n",
            "\n",
            "Clont:\n",
            "Son, come, have not thramise mucke to take On.\n",
            "\n",
            "MARCIUS:\n",
            "My Lord, aftul hath.\n",
            "\n",
            "Pare that a endway, my feanst, and In heavent, tof it her!\n",
            "Am Wherel that misters, have latise in ov the house long:\n",
            "Will absen liking me his husbale him spear; and all, yet love.\n",
            "I camones, and whom the the lame\n",
            "laon,---on her evicks eye mys, and The hope his so.\n",
            "\n",
            "ANGELO:\n",
            "That hanks the find;\n",
            "And hanks moure inled at than Pring my offer my lord\n",
            "At you saw adate the Eart to diven to marcie\n",
            "Him-his chan the would mean to Rour.\n",
            "And your gare mess, and his me? what there threas mays\n",
            "When he wintle the dead of courfenyy stizech; lord\n",
            "Evian he lap in stroked pardones oven:\n",
            "He parscies our fly vittless so upon: you fear tas not consle myself?\n",
            "What, is I condemntly and throbear\n",
            "that have unces me of my hourl name to so lack.\n",
            "\n",
            "Prive, much iI subject.\n",
            "\n",
            "KING HENRY Given with thy son\n",
            "As of time the should eyes man apprian'd what than the must peace,\n",
            "Sike to the han the ibed twatch time wouth, the counter.\n",
            "\n",
            "GLOUCESTER:\n",
            "My lord.\n",
            "\n",
            "First Qulence; get men the vouch corn.\n",
            "\n",
            "Kand Sand:\n",
            "Way have war then, mass thinks so tears\n",
            "Singing the veison him a would was\n",
            "The browgued has anI hast a cannot 'timal, the are inkeed,\n",
            "Alamonaties braffleing brothn they have ear wife\n",
            "Bettise thou follo! where portle;\n",
            "As there colian end jurk the vown;\n",
            "Maurth?\n",
            "\n",
            "Shall In subs night, stay, belasty Whou's loven absolams be:\n",
            "Have think buy no to gake the ban;\n",
            "I now was him none to may meet,e you therein find.\n",
            "\n",
            "ISHONRBY:\n",
            "And ulare thee, sign of my soul ble-dark sains some had thou ave, scase:\n",
            "Out proveted when see we have in\n",
            "More Lorced.\n",
            "\n",
            "CLARENCE:\n",
            "Thy patrieves him\n",
            "'Twill shame rish unam?\n",
            "\n",
            "Jush II:\n",
            "Sirt all, cours him, byod, Courightelings.\n",
            "But He harn Well, think love-ming a than accuses cold, of more 'chollater hath twretch'd may in a gain.\n",
            "\n",
            "KOF Y:\n",
            "Ay have of i'll him sputict of hither;\n",
            "I am was than as not swouly purs;\n",
            "Out you wise thou arbatua no of to,\n",
            "The hasparing to six and Should what me such moten's bride!\n",
            "\n",
            "\n",
            "MERC\n"
          ]
        }
      ]
    }
  ]
}