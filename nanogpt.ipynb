{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- [prepare.py](https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py), [model.py](https://github.com/karpathy/nanoGPT/blob/master/model.py), [train.py](https://github.com/karpathy/nanoGPT/blob/master/train.py), [sample.py](https://github.com/karpathy/nanoGPT/blob/master/sample.py)"
      ],
      "metadata": {
        "id": "8nMX7hXOEEMm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngJO9qJApA5S"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (multi-head) attention module and its dims\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "B, T, C = 2, 3, 4 # batch size, seq len, n_embd\n",
        "x = torch.randn(B, T, C)\n",
        "print(f\"x dims: {x.shape}\\n\") # (B, T, C)\n",
        "\n",
        "c_attn = torch.randn(C, 3 * C) # W_q, W_k, W_v concatted into one\n",
        "print(f\"c_attn dims: {c_attn.shape}\\n\") # (C, 3C)\n",
        "\n",
        "qkv = torch.matmul(x, c_attn)\n",
        "print(f\"x @ c_attn dims: {qkv.shape}\\n\") # (B, T, 3C)\n",
        "\n",
        "q, k, v = qkv.split(C, dim=2)\n",
        "print(f\"q dims: {q.shape}\") # (B, T, C)\n",
        "print(f\"{q[0][0]}\\n\")\n",
        "\n",
        "nh = 2  # num heads\n",
        "assert C % nh == 0\n",
        "# from 3d tensor to 4d tensor\n",
        "q = q.view(B, T, nh, C//nh) # nh = num heads = H, C//nh = head size = Q\n",
        "k = k.view(B, T, nh, C//nh)\n",
        "v = v.view(B, T, nh, C//nh)\n",
        "# first C//nh elements of the 1d tensor becomes the first row of the 2d tensor\n",
        "print(f\"q dims: {q.shape}\")  # (B, T, nh, head size)\n",
        "print(f\"{q[0][0]}\\n\")\n",
        "\n",
        "q = q.transpose(1, 2) # (B, nh, T, hs)\n",
        "k = k.transpose(1, 2) # (B, nh, T, hs)\n",
        "v = v.transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "# att = q @ k.T\n",
        "att = q @ k.transpose(-2, -1) # (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nh, T, T)\n",
        "print(f\"att dims: {att.shape}\\n\") # (B, nh, T, T)\n",
        "# att = att / sqrt(k.shape(-1))\n",
        "att = att * (1.0 / math.sqrt(k.size(-1)))\n",
        "# att = softmax(att)\n",
        "att = F.softmax(att, dim=-1)\n",
        "# y = att @ v\n",
        "y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "print(f\"y dims: {y.shape}\\n\") # (B, nh, T, hs)\n",
        "\n",
        "y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "print(f\"y dims: {y.shape}\") # (B, T, C)\n",
        "\n",
        "# CausalSelfAttention: (B, T, C) in, (B, T, C) out\n",
        "# MLP: (B, T, C) in, (B, T, C) out\n",
        "# Block: (B, T, C) in, (B, T, C) out\n",
        "\n",
        "# block_size (seq len) = 1024\n",
        "# n_embd = 768\n",
        "# n_head = 12\n",
        "# n_layer (num blocks) = 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csFxbYVUsbId",
        "outputId": "da0ade0c-12f3-4267-8880-c5b967e639b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x dims: torch.Size([2, 3, 4])\n",
            "\n",
            "c_attn dims: torch.Size([4, 12])\n",
            "\n",
            "x @ c_attn dims: torch.Size([2, 3, 12])\n",
            "\n",
            "q dims: torch.Size([2, 3, 4])\n",
            "tensor([-1.9947, -1.8315, -1.0380,  0.9199])\n",
            "\n",
            "q dims: torch.Size([2, 3, 2, 2])\n",
            "tensor([[-1.9947, -1.8315],\n",
            "        [-1.0380,  0.9199]])\n",
            "\n",
            "att dims: torch.Size([2, 2, 3, 3])\n",
            "\n",
            "y dims: torch.Size([2, 2, 3, 2])\n",
            "\n",
            "y dims: torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py\n",
        "\n",
        "\"\"\"\n",
        "Prepare the Shakespeare dataset for character-level language modeling.\n",
        "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
        "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
        "encoder and decoder and some other related info.\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.getcwd(), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "data_dir = os.path.join(os.getcwd(), 'data/shakespeare_char/')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_dir, 'val.bin'))\n",
        "\n",
        "# save the meta information as well, to help us encode/decode later\n",
        "meta = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'itos': itos,\n",
        "    'stoi': stoi,\n",
        "}\n",
        "with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
        "    pickle.dump(meta, f)\n",
        "\n",
        "# length of dataset in characters: 1,115,394\n",
        "# all the unique characters:\n",
        "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
        "# vocab size: 65\n",
        "# train has 1,003,854 tokens\n",
        "# val has 111,540 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSf1rBuypEZ0",
        "outputId": "8c95a635-8514-472f-957a-1c1f193c97de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
        "\n",
        "\"\"\"\n",
        "Full definition of a GPT Language Model, all of it in this single file.\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim)) # initialized as ones\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # initialized as zeros\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 # just to make sure\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        # hasattr(object, attribute): object is the object you're checking\n",
        "        # and attribute is a string representing the name of the attribute you want to check for\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # stands for language model head\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module): # normal distribution with mean 0.0, std 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h: # h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x) # ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond) # forward is a specially designated function\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature # so this is where and how temperature is used\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "wVxF1bdnEIjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/karpathy/nanoGPT/blob/master/train.py\n",
        "\n",
        "\"\"\"\n",
        "This training script can be run both on a single gpu in debug mode,\n",
        "and also in a larger training run with distributed data parallel (ddp).\n",
        "\n",
        "To run on a single GPU, example:\n",
        "$ python train.py --batch_size=32 --compile=False\n",
        "\n",
        "To run with DDP on 4 gpus on 1 node, example:\n",
        "$ torchrun --standalone --nproc_per_node=4 train.py\n",
        "\n",
        "To run with DDP on 4 gpus across 2 nodes, example:\n",
        "- Run on the first (master) node with example IP 123.456.123.456:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "- Run on the worker node:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "# from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "# config/train_shakespeare_char.py\n",
        "\n",
        "# train a miniature character-level shakespeare model\n",
        "# good for debugging and playing on macbooks and such\n",
        "\n",
        "out_dir = 'out-shakespeare-char'\n",
        "eval_interval = 250 # keep frequent because we'll overfit\n",
        "eval_iters = 200\n",
        "log_interval = 10 # don't print too too often\n",
        "\n",
        "# we expect to overfit on this small dataset, so only save when val improves\n",
        "always_save_checkpoint = False\n",
        "\n",
        "wandb_log = False # override via command line if you like\n",
        "wandb_project = 'shakespeare-char'\n",
        "wandb_run_name = 'mini-gpt'\n",
        "\n",
        "dataset = 'shakespeare_char'\n",
        "gradient_accumulation_steps = 1\n",
        "batch_size = 64\n",
        "block_size = 256 # context of up to 256 previous characters\n",
        "\n",
        "# baby GPT model :)\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "\n",
        "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
        "max_iters = 5000\n",
        "lr_decay_iters = 5000 # make equal to max_iters usually\n",
        "min_lr = 1e-4 # learning_rate / 10 usually\n",
        "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
        "\n",
        "warmup_iters = 100 # not super necessary potentially\n",
        "\n",
        "# on macbook also add\n",
        "# device = 'cpu'  # run on cpu only\n",
        "# compile = False # do not torch compile the model\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY3Phhw8-53x",
        "outputId": "24138bfc-bc81-4376-9dea-731a121213d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-10-15 07:57:38,792] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:39,273] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:40,102] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:40,413] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:40,859] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:41,151] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:41,589] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:41,884] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:42,507] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:42,801] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:43,248] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-10-15 07:57:43,562] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 4.2716, time 23774.77ms, mfu -100.00%\n",
            "iter 10: loss 3.2437, time 24.24ms, mfu 15.37%\n",
            "iter 20: loss 2.7946, time 22.79ms, mfu 15.47%\n",
            "iter 30: loss 2.6424, time 22.80ms, mfu 15.56%\n",
            "iter 40: loss 2.5812, time 22.72ms, mfu 15.64%\n",
            "iter 50: loss 2.5297, time 22.92ms, mfu 15.70%\n",
            "iter 60: loss 2.5140, time 23.03ms, mfu 15.75%\n",
            "iter 70: loss 2.4996, time 22.67ms, mfu 15.82%\n",
            "iter 80: loss 2.4946, time 22.52ms, mfu 15.89%\n",
            "iter 90: loss 2.4626, time 23.21ms, mfu 15.91%\n",
            "iter 100: loss 2.4628, time 22.79ms, mfu 15.95%\n",
            "iter 110: loss 2.4548, time 22.48ms, mfu 16.01%\n",
            "iter 120: loss 2.4346, time 22.56ms, mfu 16.06%\n",
            "iter 130: loss 2.4120, time 22.86ms, mfu 16.09%\n",
            "iter 140: loss 2.4006, time 22.79ms, mfu 16.11%\n",
            "iter 150: loss 2.4073, time 22.48ms, mfu 16.16%\n",
            "iter 160: loss 2.3713, time 22.58ms, mfu 16.19%\n",
            "iter 170: loss 2.3619, time 22.66ms, mfu 16.22%\n",
            "iter 180: loss 2.3161, time 22.44ms, mfu 16.26%\n",
            "iter 190: loss 2.2666, time 22.40ms, mfu 16.30%\n",
            "iter 200: loss 2.2111, time 22.40ms, mfu 16.33%\n",
            "iter 210: loss 2.1446, time 22.48ms, mfu 16.35%\n",
            "iter 220: loss 2.1471, time 23.41ms, mfu 16.31%\n",
            "iter 230: loss 2.0719, time 22.34ms, mfu 16.35%\n",
            "iter 240: loss 2.0834, time 22.59ms, mfu 16.36%\n",
            "step 250: train loss 1.9734, val loss 2.0773\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0386, time 6184.12ms, mfu 14.73%\n",
            "iter 260: loss 1.9853, time 22.65ms, mfu 14.90%\n",
            "iter 270: loss 1.9714, time 22.62ms, mfu 15.06%\n",
            "iter 280: loss 1.9883, time 22.60ms, mfu 15.20%\n",
            "iter 290: loss 1.9218, time 22.64ms, mfu 15.33%\n",
            "iter 300: loss 1.9098, time 22.90ms, mfu 15.42%\n",
            "iter 310: loss 1.8757, time 22.47ms, mfu 15.54%\n",
            "iter 320: loss 1.8556, time 22.89ms, mfu 15.61%\n",
            "iter 330: loss 1.8258, time 22.59ms, mfu 15.70%\n",
            "iter 340: loss 1.7901, time 22.46ms, mfu 15.79%\n",
            "iter 350: loss 1.8356, time 22.55ms, mfu 15.86%\n",
            "iter 360: loss 1.7796, time 22.59ms, mfu 15.93%\n",
            "iter 370: loss 1.7461, time 22.58ms, mfu 15.98%\n",
            "iter 380: loss 1.7347, time 22.97ms, mfu 16.01%\n",
            "iter 390: loss 1.7419, time 22.66ms, mfu 16.05%\n",
            "iter 400: loss 1.7637, time 24.44ms, mfu 15.97%\n",
            "iter 410: loss 1.6998, time 22.49ms, mfu 16.03%\n",
            "iter 420: loss 1.7179, time 22.62ms, mfu 16.08%\n",
            "iter 430: loss 1.6872, time 22.80ms, mfu 16.10%\n",
            "iter 440: loss 1.6616, time 22.55ms, mfu 16.14%\n",
            "iter 450: loss 1.6560, time 22.44ms, mfu 16.19%\n",
            "iter 460: loss 1.6127, time 23.13ms, mfu 16.18%\n",
            "iter 470: loss 1.6641, time 22.36ms, mfu 16.23%\n",
            "iter 480: loss 1.6227, time 22.79ms, mfu 16.24%\n",
            "iter 490: loss 1.6020, time 22.64ms, mfu 16.26%\n",
            "step 500: train loss 1.5268, val loss 1.7353\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6026, time 3283.84ms, mfu 14.65%\n",
            "iter 510: loss 1.6150, time 22.85ms, mfu 14.82%\n",
            "iter 520: loss 1.5996, time 23.11ms, mfu 14.95%\n",
            "iter 530: loss 1.5647, time 23.02ms, mfu 15.07%\n",
            "iter 540: loss 1.6229, time 22.78ms, mfu 15.20%\n",
            "iter 550: loss 1.5634, time 22.90ms, mfu 15.31%\n",
            "iter 560: loss 1.5710, time 24.72ms, mfu 15.28%\n",
            "iter 570: loss 1.5661, time 22.44ms, mfu 15.42%\n",
            "iter 580: loss 1.5378, time 22.87ms, mfu 15.50%\n",
            "iter 590: loss 1.4920, time 23.08ms, mfu 15.57%\n",
            "iter 600: loss 1.5139, time 22.78ms, mfu 15.65%\n",
            "iter 610: loss 1.5446, time 22.44ms, mfu 15.74%\n",
            "iter 620: loss 1.5312, time 23.43ms, mfu 15.76%\n",
            "iter 630: loss 1.5092, time 23.33ms, mfu 15.78%\n",
            "iter 640: loss 1.4702, time 23.01ms, mfu 15.82%\n",
            "iter 650: loss 1.4978, time 22.85ms, mfu 15.87%\n",
            "iter 660: loss 1.5113, time 22.87ms, mfu 15.91%\n",
            "iter 670: loss 1.4482, time 22.74ms, mfu 15.96%\n",
            "iter 680: loss 1.5071, time 24.33ms, mfu 15.89%\n",
            "iter 690: loss 1.4604, time 24.57ms, mfu 15.82%\n",
            "iter 700: loss 1.4826, time 22.95ms, mfu 15.86%\n",
            "iter 710: loss 1.4537, time 23.74ms, mfu 15.85%\n",
            "iter 720: loss 1.4414, time 22.76ms, mfu 15.90%\n",
            "iter 730: loss 1.4275, time 22.64ms, mfu 15.95%\n",
            "iter 740: loss 1.4278, time 23.44ms, mfu 15.95%\n",
            "step 750: train loss 1.3603, val loss 1.5941\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4195, time 3257.55ms, mfu 14.37%\n",
            "iter 760: loss 1.4508, time 22.56ms, mfu 14.58%\n",
            "iter 770: loss 1.4277, time 22.55ms, mfu 14.77%\n",
            "iter 780: loss 1.4207, time 22.49ms, mfu 14.95%\n",
            "iter 790: loss 1.4182, time 22.68ms, mfu 15.10%\n",
            "iter 800: loss 1.4272, time 22.58ms, mfu 15.24%\n",
            "iter 810: loss 1.4066, time 22.55ms, mfu 15.37%\n",
            "iter 820: loss 1.4140, time 22.74ms, mfu 15.47%\n",
            "iter 830: loss 1.3969, time 22.67ms, mfu 15.57%\n",
            "iter 840: loss 1.4013, time 22.84ms, mfu 15.64%\n",
            "iter 850: loss 1.3896, time 22.94ms, mfu 15.70%\n",
            "iter 860: loss 1.3900, time 22.73ms, mfu 15.77%\n",
            "iter 870: loss 1.3944, time 22.85ms, mfu 15.83%\n",
            "iter 880: loss 1.3795, time 22.55ms, mfu 15.90%\n",
            "iter 890: loss 1.3847, time 23.14ms, mfu 15.92%\n",
            "iter 900: loss 1.3712, time 23.02ms, mfu 15.94%\n",
            "iter 910: loss 1.3238, time 22.54ms, mfu 16.00%\n",
            "iter 920: loss 1.3641, time 22.63ms, mfu 16.05%\n",
            "iter 930: loss 1.3618, time 22.63ms, mfu 16.09%\n",
            "iter 940: loss 1.3458, time 22.67ms, mfu 16.12%\n",
            "iter 950: loss 1.3584, time 22.92ms, mfu 16.14%\n",
            "iter 960: loss 1.3628, time 22.41ms, mfu 16.19%\n",
            "iter 970: loss 1.3658, time 22.77ms, mfu 16.20%\n",
            "iter 980: loss 1.3571, time 22.68ms, mfu 16.23%\n",
            "iter 990: loss 1.3401, time 22.60ms, mfu 16.25%\n",
            "step 1000: train loss 1.2747, val loss 1.5221\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3363, time 3283.25ms, mfu 14.64%\n",
            "iter 1010: loss 1.3323, time 22.89ms, mfu 14.80%\n",
            "iter 1020: loss 1.3164, time 22.69ms, mfu 14.96%\n",
            "iter 1030: loss 1.3308, time 22.81ms, mfu 15.10%\n",
            "iter 1040: loss 1.3527, time 22.62ms, mfu 15.24%\n",
            "iter 1050: loss 1.2914, time 22.72ms, mfu 15.36%\n",
            "iter 1060: loss 1.3334, time 22.55ms, mfu 15.47%\n",
            "iter 1070: loss 1.3303, time 22.69ms, mfu 15.57%\n",
            "iter 1080: loss 1.3364, time 22.47ms, mfu 15.67%\n",
            "iter 1090: loss 1.3575, time 22.89ms, mfu 15.73%\n",
            "iter 1100: loss 1.3199, time 22.64ms, mfu 15.80%\n",
            "iter 1110: loss 1.2974, time 23.05ms, mfu 15.84%\n",
            "iter 1120: loss 1.2964, time 22.68ms, mfu 15.90%\n",
            "iter 1130: loss 1.2978, time 22.69ms, mfu 15.95%\n",
            "iter 1140: loss 1.3025, time 22.77ms, mfu 15.99%\n",
            "iter 1150: loss 1.3060, time 22.74ms, mfu 16.03%\n",
            "iter 1160: loss 1.3298, time 22.56ms, mfu 16.08%\n",
            "iter 1170: loss 1.2945, time 22.78ms, mfu 16.11%\n",
            "iter 1180: loss 1.3195, time 23.56ms, mfu 16.08%\n",
            "iter 1190: loss 1.2760, time 23.00ms, mfu 16.09%\n",
            "iter 1200: loss 1.2882, time 23.11ms, mfu 16.09%\n",
            "iter 1210: loss 1.2656, time 23.01ms, mfu 16.10%\n",
            "iter 1220: loss 1.2994, time 22.82ms, mfu 16.13%\n",
            "iter 1230: loss 1.2969, time 23.27ms, mfu 16.12%\n",
            "iter 1240: loss 1.2943, time 22.62ms, mfu 16.15%\n",
            "step 1250: train loss 1.2054, val loss 1.4958\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2776, time 3273.81ms, mfu 14.55%\n",
            "iter 1260: loss 1.2814, time 22.58ms, mfu 14.74%\n",
            "iter 1270: loss 1.2622, time 22.71ms, mfu 14.91%\n",
            "iter 1280: loss 1.2576, time 22.75ms, mfu 15.06%\n",
            "iter 1290: loss 1.2810, time 22.50ms, mfu 15.21%\n",
            "iter 1300: loss 1.2989, time 22.77ms, mfu 15.32%\n",
            "iter 1310: loss 1.2359, time 22.37ms, mfu 15.46%\n",
            "iter 1320: loss 1.3034, time 22.55ms, mfu 15.56%\n",
            "iter 1330: loss 1.2690, time 22.59ms, mfu 15.66%\n",
            "iter 1340: loss 1.3033, time 23.17ms, mfu 15.70%\n",
            "iter 1350: loss 1.2627, time 22.74ms, mfu 15.77%\n",
            "iter 1360: loss 1.2737, time 22.55ms, mfu 15.84%\n",
            "iter 1370: loss 1.2492, time 22.56ms, mfu 15.91%\n",
            "iter 1380: loss 1.2732, time 22.78ms, mfu 15.95%\n",
            "iter 1390: loss 1.2454, time 22.74ms, mfu 16.00%\n",
            "iter 1400: loss 1.2602, time 22.96ms, mfu 16.02%\n",
            "iter 1410: loss 1.2521, time 22.92ms, mfu 16.04%\n",
            "iter 1420: loss 1.2705, time 22.58ms, mfu 16.09%\n",
            "iter 1430: loss 1.2412, time 22.62ms, mfu 16.13%\n",
            "iter 1440: loss 1.2510, time 22.73ms, mfu 16.15%\n",
            "iter 1450: loss 1.2276, time 22.78ms, mfu 16.17%\n",
            "iter 1460: loss 1.2369, time 23.13ms, mfu 16.17%\n",
            "iter 1470: loss 1.2189, time 22.79ms, mfu 16.19%\n",
            "iter 1480: loss 1.2045, time 22.96ms, mfu 16.19%\n",
            "iter 1490: loss 1.2322, time 23.14ms, mfu 16.18%\n",
            "step 1500: train loss 1.1544, val loss 1.4908\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1848, time 3266.55ms, mfu 14.57%\n",
            "iter 1510: loss 1.2313, time 22.74ms, mfu 14.76%\n",
            "iter 1520: loss 1.2283, time 22.66ms, mfu 14.92%\n",
            "iter 1530: loss 1.2595, time 22.71ms, mfu 15.07%\n",
            "iter 1540: loss 1.1932, time 29.20ms, mfu 14.84%\n",
            "iter 1550: loss 1.2227, time 22.45ms, mfu 15.02%\n",
            "iter 1560: loss 1.2041, time 23.08ms, mfu 15.13%\n",
            "iter 1570: loss 1.2368, time 22.83ms, mfu 15.25%\n",
            "iter 1580: loss 1.2051, time 22.77ms, mfu 15.36%\n",
            "iter 1590: loss 1.1894, time 22.75ms, mfu 15.46%\n",
            "iter 1600: loss 1.1965, time 22.86ms, mfu 15.55%\n",
            "iter 1610: loss 1.2439, time 22.63ms, mfu 15.64%\n",
            "iter 1620: loss 1.1787, time 22.98ms, mfu 15.70%\n",
            "iter 1630: loss 1.2004, time 36.72ms, mfu 15.14%\n",
            "iter 1640: loss 1.2021, time 22.74ms, mfu 15.27%\n",
            "iter 1650: loss 1.1840, time 22.75ms, mfu 15.38%\n",
            "iter 1660: loss 1.2154, time 22.83ms, mfu 15.47%\n",
            "iter 1670: loss 1.1974, time 23.00ms, mfu 15.54%\n",
            "iter 1680: loss 1.2017, time 22.88ms, mfu 15.62%\n",
            "iter 1690: loss 1.2027, time 22.86ms, mfu 15.69%\n",
            "iter 1700: loss 1.1868, time 22.77ms, mfu 15.75%\n",
            "iter 1710: loss 1.1808, time 22.53ms, mfu 15.83%\n",
            "iter 1720: loss 1.1788, time 22.56ms, mfu 15.90%\n",
            "iter 1730: loss 1.1953, time 22.63ms, mfu 15.96%\n",
            "iter 1740: loss 1.1666, time 23.01ms, mfu 15.98%\n",
            "step 1750: train loss 1.1043, val loss 1.4708\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1856, time 3275.70ms, mfu 14.39%\n",
            "iter 1760: loss 1.1870, time 23.07ms, mfu 14.57%\n",
            "iter 1770: loss 1.1974, time 23.00ms, mfu 14.73%\n",
            "iter 1780: loss 1.1913, time 22.84ms, mfu 14.89%\n",
            "iter 1790: loss 1.1936, time 24.81ms, mfu 14.90%\n",
            "iter 1800: loss 1.1774, time 24.10ms, mfu 14.96%\n",
            "iter 1810: loss 1.1610, time 22.37ms, mfu 15.13%\n",
            "iter 1820: loss 1.1604, time 23.26ms, mfu 15.22%\n",
            "iter 1830: loss 1.1662, time 24.38ms, mfu 15.23%\n",
            "iter 1840: loss 1.1630, time 22.57ms, mfu 15.35%\n",
            "iter 1850: loss 1.1568, time 22.86ms, mfu 15.45%\n",
            "iter 1860: loss 1.1754, time 23.96ms, mfu 15.46%\n",
            "iter 1870: loss 1.1425, time 23.34ms, mfu 15.51%\n",
            "iter 1880: loss 1.1826, time 23.00ms, mfu 15.58%\n",
            "iter 1890: loss 1.1789, time 22.76ms, mfu 15.66%\n",
            "iter 1900: loss 1.1366, time 25.12ms, mfu 15.58%\n",
            "iter 1910: loss 1.1703, time 22.64ms, mfu 15.66%\n",
            "iter 1920: loss 1.1712, time 23.57ms, mfu 15.68%\n",
            "iter 1930: loss 1.1443, time 22.68ms, mfu 15.75%\n",
            "iter 1940: loss 1.1285, time 22.56ms, mfu 15.83%\n",
            "iter 1950: loss 1.1339, time 22.81ms, mfu 15.88%\n",
            "iter 1960: loss 1.1506, time 22.59ms, mfu 15.94%\n",
            "iter 1970: loss 1.1565, time 22.56ms, mfu 16.00%\n",
            "iter 1980: loss 1.1460, time 22.67ms, mfu 16.04%\n",
            "iter 1990: loss 1.1529, time 22.95ms, mfu 16.06%\n",
            "step 2000: train loss 1.0562, val loss 1.4849\n",
            "iter 2000: loss 1.1280, time 3025.06ms, mfu 14.47%\n",
            "iter 2010: loss 1.1251, time 22.73ms, mfu 14.66%\n",
            "iter 2020: loss 1.1203, time 23.31ms, mfu 14.79%\n",
            "iter 2030: loss 1.1530, time 22.83ms, mfu 14.95%\n",
            "iter 2040: loss 1.1407, time 27.64ms, mfu 14.80%\n",
            "iter 2050: loss 1.1066, time 23.08ms, mfu 14.93%\n",
            "iter 2060: loss 1.0996, time 22.67ms, mfu 15.08%\n",
            "iter 2070: loss 1.1188, time 31.17ms, mfu 14.77%\n",
            "iter 2080: loss 1.1254, time 22.61ms, mfu 14.94%\n",
            "iter 2090: loss 1.1302, time 22.58ms, mfu 15.10%\n",
            "iter 2100: loss 1.1304, time 22.58ms, mfu 15.24%\n",
            "iter 2110: loss 1.1343, time 22.66ms, mfu 15.36%\n",
            "iter 2120: loss 1.1281, time 22.55ms, mfu 15.48%\n",
            "iter 2130: loss 1.1384, time 22.49ms, mfu 15.59%\n",
            "iter 2140: loss 1.1389, time 22.56ms, mfu 15.68%\n",
            "iter 2150: loss 1.1280, time 23.00ms, mfu 15.73%\n",
            "iter 2160: loss 1.1502, time 23.71ms, mfu 15.73%\n",
            "iter 2170: loss 1.1344, time 22.33ms, mfu 15.82%\n",
            "iter 2180: loss 1.1070, time 23.12ms, mfu 15.85%\n",
            "iter 2190: loss 1.1103, time 22.78ms, mfu 15.90%\n",
            "iter 2200: loss 1.1210, time 22.98ms, mfu 15.94%\n",
            "iter 2210: loss 1.1064, time 22.91ms, mfu 15.97%\n",
            "iter 2220: loss 1.1105, time 22.87ms, mfu 16.00%\n",
            "iter 2230: loss 1.1189, time 23.04ms, mfu 16.02%\n",
            "iter 2240: loss 1.1218, time 22.85ms, mfu 16.05%\n",
            "step 2250: train loss 1.0112, val loss 1.4902\n",
            "iter 2250: loss 1.1048, time 3018.80ms, mfu 14.46%\n",
            "iter 2260: loss 1.1083, time 22.22ms, mfu 14.69%\n",
            "iter 2270: loss 1.1247, time 23.32ms, mfu 14.82%\n",
            "iter 2280: loss 1.0937, time 22.36ms, mfu 15.00%\n",
            "iter 2290: loss 1.1357, time 22.86ms, mfu 15.13%\n",
            "iter 2300: loss 1.1213, time 22.45ms, mfu 15.28%\n",
            "iter 2310: loss 1.0883, time 23.25ms, mfu 15.35%\n",
            "iter 2320: loss 1.0955, time 22.41ms, mfu 15.48%\n",
            "iter 2330: loss 1.0938, time 22.55ms, mfu 15.58%\n",
            "iter 2340: loss 1.1164, time 22.40ms, mfu 15.69%\n",
            "iter 2350: loss 1.1055, time 22.67ms, mfu 15.76%\n",
            "iter 2360: loss 1.1082, time 23.36ms, mfu 15.78%\n",
            "iter 2370: loss 1.0864, time 22.83ms, mfu 15.84%\n",
            "iter 2380: loss 1.0864, time 23.82ms, mfu 15.82%\n",
            "iter 2390: loss 1.0857, time 22.40ms, mfu 15.90%\n",
            "iter 2400: loss 1.0771, time 24.96ms, mfu 15.80%\n",
            "iter 2410: loss 1.0640, time 22.96ms, mfu 15.84%\n",
            "iter 2420: loss 1.0798, time 23.15ms, mfu 15.87%\n",
            "iter 2430: loss 1.0619, time 22.46ms, mfu 15.94%\n",
            "iter 2440: loss 1.0572, time 22.46ms, mfu 16.01%\n",
            "iter 2450: loss 1.0723, time 22.67ms, mfu 16.05%\n",
            "iter 2460: loss 1.0820, time 22.74ms, mfu 16.08%\n",
            "iter 2470: loss 1.0875, time 22.70ms, mfu 16.12%\n",
            "iter 2480: loss 1.0819, time 23.43ms, mfu 16.09%\n",
            "iter 2490: loss 1.0552, time 22.59ms, mfu 16.13%\n",
            "step 2500: train loss 0.9619, val loss 1.4966\n",
            "iter 2500: loss 1.0869, time 3016.46ms, mfu 14.53%\n",
            "iter 2510: loss 1.0632, time 22.69ms, mfu 14.72%\n",
            "iter 2520: loss 1.0537, time 22.74ms, mfu 14.89%\n",
            "iter 2530: loss 1.0563, time 22.78ms, mfu 15.04%\n",
            "iter 2540: loss 1.0527, time 23.13ms, mfu 15.14%\n",
            "iter 2550: loss 1.0685, time 22.87ms, mfu 15.26%\n",
            "iter 2560: loss 1.0532, time 22.91ms, mfu 15.36%\n",
            "iter 2570: loss 1.0759, time 22.75ms, mfu 15.46%\n",
            "iter 2580: loss 1.0706, time 23.11ms, mfu 15.53%\n",
            "iter 2590: loss 1.0580, time 23.05ms, mfu 15.59%\n",
            "iter 2600: loss 1.0616, time 22.81ms, mfu 15.66%\n",
            "iter 2610: loss 1.0485, time 22.66ms, mfu 15.74%\n",
            "iter 2620: loss 1.0460, time 22.76ms, mfu 15.81%\n",
            "iter 2630: loss 1.0177, time 22.99ms, mfu 15.85%\n",
            "iter 2640: loss 1.0426, time 23.03ms, mfu 15.88%\n",
            "iter 2650: loss 1.0654, time 25.04ms, mfu 15.78%\n",
            "iter 2660: loss 1.0315, time 23.02ms, mfu 15.82%\n",
            "iter 2670: loss 1.0189, time 22.67ms, mfu 15.88%\n",
            "iter 2680: loss 1.0497, time 22.56ms, mfu 15.95%\n",
            "iter 2690: loss 1.0466, time 22.75ms, mfu 15.99%\n",
            "iter 2700: loss 1.0134, time 22.48ms, mfu 16.05%\n",
            "iter 2710: loss 1.0318, time 22.82ms, mfu 16.08%\n",
            "iter 2720: loss 1.0374, time 22.83ms, mfu 16.10%\n",
            "iter 2730: loss 1.0621, time 22.60ms, mfu 16.14%\n",
            "iter 2740: loss 1.0225, time 22.62ms, mfu 16.17%\n",
            "step 2750: train loss 0.9141, val loss 1.5230\n",
            "iter 2750: loss 1.0355, time 3011.29ms, mfu 14.57%\n",
            "iter 2760: loss 1.0268, time 22.15ms, mfu 14.79%\n",
            "iter 2770: loss 1.0226, time 22.29ms, mfu 14.99%\n",
            "iter 2780: loss 1.0235, time 22.31ms, mfu 15.16%\n",
            "iter 2790: loss 1.0345, time 22.29ms, mfu 15.31%\n",
            "iter 2800: loss 1.0062, time 22.82ms, mfu 15.42%\n",
            "iter 2810: loss 1.0412, time 22.49ms, mfu 15.53%\n",
            "iter 2820: loss 1.0219, time 22.57ms, mfu 15.63%\n",
            "iter 2830: loss 1.0277, time 23.16ms, mfu 15.67%\n",
            "iter 2840: loss 0.9943, time 22.97ms, mfu 15.73%\n",
            "iter 2850: loss 1.0276, time 22.47ms, mfu 15.81%\n",
            "iter 2860: loss 1.0187, time 22.86ms, mfu 15.86%\n",
            "iter 2870: loss 1.0075, time 22.50ms, mfu 15.93%\n",
            "iter 2880: loss 1.0284, time 22.74ms, mfu 15.98%\n",
            "iter 2890: loss 1.0038, time 22.69ms, mfu 16.02%\n",
            "iter 2900: loss 0.9890, time 22.46ms, mfu 16.08%\n",
            "iter 2910: loss 1.0428, time 22.41ms, mfu 16.13%\n",
            "iter 2920: loss 1.0107, time 22.54ms, mfu 16.17%\n",
            "iter 2930: loss 0.9970, time 22.65ms, mfu 16.20%\n",
            "iter 2940: loss 0.9920, time 22.47ms, mfu 16.24%\n",
            "iter 2950: loss 1.0133, time 22.58ms, mfu 16.27%\n",
            "iter 2960: loss 0.9910, time 22.69ms, mfu 16.28%\n",
            "iter 2970: loss 0.9914, time 23.42ms, mfu 16.24%\n",
            "iter 2980: loss 0.9994, time 22.59ms, mfu 16.27%\n",
            "iter 2990: loss 0.9800, time 22.50ms, mfu 16.30%\n",
            "step 3000: train loss 0.8646, val loss 1.5343\n",
            "iter 3000: loss 0.9896, time 3016.28ms, mfu 14.68%\n",
            "iter 3010: loss 0.9934, time 22.94ms, mfu 14.84%\n",
            "iter 3020: loss 0.9994, time 22.54ms, mfu 15.01%\n",
            "iter 3030: loss 0.9994, time 22.50ms, mfu 15.16%\n",
            "iter 3040: loss 1.0129, time 22.63ms, mfu 15.29%\n",
            "iter 3050: loss 0.9740, time 22.94ms, mfu 15.39%\n",
            "iter 3060: loss 1.0012, time 23.04ms, mfu 15.47%\n",
            "iter 3070: loss 1.0164, time 25.11ms, mfu 15.40%\n",
            "iter 3080: loss 0.9999, time 22.56ms, mfu 15.51%\n",
            "iter 3090: loss 0.9814, time 22.81ms, mfu 15.60%\n",
            "iter 3100: loss 0.9953, time 22.73ms, mfu 15.68%\n",
            "iter 3110: loss 0.9706, time 23.38ms, mfu 15.70%\n",
            "iter 3120: loss 0.9934, time 22.76ms, mfu 15.77%\n",
            "iter 3130: loss 0.9738, time 22.78ms, mfu 15.83%\n",
            "iter 3140: loss 0.9824, time 23.07ms, mfu 15.86%\n",
            "iter 3150: loss 0.9930, time 22.87ms, mfu 15.90%\n",
            "iter 3160: loss 1.0076, time 23.44ms, mfu 15.90%\n",
            "iter 3170: loss 0.9683, time 23.35ms, mfu 15.91%\n",
            "iter 3180: loss 0.9713, time 25.99ms, mfu 15.75%\n",
            "iter 3190: loss 0.9897, time 24.42ms, mfu 15.70%\n",
            "iter 3200: loss 0.9660, time 22.96ms, mfu 15.75%\n",
            "iter 3210: loss 0.9634, time 22.98ms, mfu 15.80%\n",
            "iter 3220: loss 0.9540, time 22.90ms, mfu 15.85%\n",
            "iter 3230: loss 0.9533, time 22.80ms, mfu 15.90%\n",
            "iter 3240: loss 0.9448, time 22.48ms, mfu 15.96%\n",
            "step 3250: train loss 0.8222, val loss 1.5641\n",
            "iter 3250: loss 0.9747, time 3011.70ms, mfu 14.38%\n",
            "iter 3260: loss 0.9572, time 24.35ms, mfu 14.47%\n",
            "iter 3270: loss 0.9641, time 23.57ms, mfu 14.61%\n",
            "iter 3280: loss 0.9469, time 22.34ms, mfu 14.81%\n",
            "iter 3290: loss 0.9452, time 22.47ms, mfu 14.99%\n",
            "iter 3300: loss 0.9413, time 22.85ms, mfu 15.12%\n",
            "iter 3310: loss 0.9497, time 22.63ms, mfu 15.26%\n",
            "iter 3320: loss 0.9627, time 22.64ms, mfu 15.38%\n",
            "iter 3330: loss 0.9539, time 22.69ms, mfu 15.48%\n",
            "iter 3340: loss 0.9461, time 22.66ms, mfu 15.58%\n",
            "iter 3350: loss 0.9524, time 22.84ms, mfu 15.65%\n",
            "iter 3360: loss 0.9292, time 22.46ms, mfu 15.75%\n",
            "iter 3370: loss 0.9570, time 22.59ms, mfu 15.82%\n",
            "iter 3380: loss 0.9448, time 22.82ms, mfu 15.87%\n",
            "iter 3390: loss 0.9443, time 22.80ms, mfu 15.92%\n",
            "iter 3400: loss 0.9513, time 22.76ms, mfu 15.96%\n",
            "iter 3410: loss 0.9434, time 22.71ms, mfu 16.01%\n",
            "iter 3420: loss 0.9470, time 22.90ms, mfu 16.03%\n",
            "iter 3430: loss 0.9484, time 22.81ms, mfu 16.06%\n",
            "iter 3440: loss 0.9704, time 22.45ms, mfu 16.12%\n",
            "iter 3450: loss 0.9517, time 22.69ms, mfu 16.15%\n",
            "iter 3460: loss 0.9446, time 22.58ms, mfu 16.18%\n",
            "iter 3470: loss 0.9409, time 22.57ms, mfu 16.22%\n",
            "iter 3480: loss 0.9475, time 22.89ms, mfu 16.22%\n",
            "iter 3490: loss 0.9130, time 22.74ms, mfu 16.24%\n",
            "step 3500: train loss 0.7788, val loss 1.5775\n",
            "iter 3500: loss 0.9092, time 3019.25ms, mfu 14.63%\n",
            "iter 3510: loss 0.9113, time 22.54ms, mfu 14.82%\n",
            "iter 3520: loss 0.9282, time 22.41ms, mfu 15.00%\n",
            "iter 3530: loss 0.9555, time 22.61ms, mfu 15.15%\n",
            "iter 3540: loss 0.9238, time 22.53ms, mfu 15.29%\n",
            "iter 3550: loss 0.9293, time 22.72ms, mfu 15.40%\n",
            "iter 3560: loss 0.9470, time 22.65ms, mfu 15.50%\n",
            "iter 3570: loss 0.9338, time 23.17ms, mfu 15.56%\n",
            "iter 3580: loss 0.9294, time 22.97ms, mfu 15.63%\n",
            "iter 3590: loss 0.9206, time 23.48ms, mfu 15.65%\n",
            "iter 3600: loss 0.9212, time 24.01ms, mfu 15.64%\n",
            "iter 3610: loss 0.9117, time 23.13ms, mfu 15.68%\n",
            "iter 3620: loss 0.8998, time 24.33ms, mfu 15.65%\n",
            "iter 3630: loss 0.9191, time 22.88ms, mfu 15.71%\n",
            "iter 3640: loss 0.9047, time 24.60ms, mfu 15.66%\n",
            "iter 3650: loss 0.9205, time 22.82ms, mfu 15.72%\n",
            "iter 3660: loss 0.9336, time 23.64ms, mfu 15.73%\n",
            "iter 3670: loss 0.9359, time 22.85ms, mfu 15.78%\n",
            "iter 3680: loss 0.9023, time 23.01ms, mfu 15.83%\n",
            "iter 3690: loss 0.9398, time 23.19ms, mfu 15.85%\n",
            "iter 3700: loss 0.8745, time 22.82ms, mfu 15.90%\n",
            "iter 3710: loss 0.8859, time 22.92ms, mfu 15.93%\n",
            "iter 3720: loss 0.8961, time 22.78ms, mfu 15.98%\n",
            "iter 3730: loss 0.8979, time 22.79ms, mfu 16.01%\n",
            "iter 3740: loss 0.9043, time 23.19ms, mfu 16.02%\n",
            "step 3750: train loss 0.7411, val loss 1.6075\n",
            "iter 3750: loss 0.9041, time 3013.00ms, mfu 14.43%\n",
            "iter 3760: loss 0.9294, time 23.29ms, mfu 14.59%\n",
            "iter 3770: loss 0.9296, time 24.86ms, mfu 14.63%\n",
            "iter 3780: loss 0.9241, time 22.86ms, mfu 14.79%\n",
            "iter 3790: loss 0.8954, time 22.34ms, mfu 14.98%\n",
            "iter 3800: loss 0.9158, time 22.56ms, mfu 15.14%\n",
            "iter 3810: loss 0.9221, time 22.77ms, mfu 15.26%\n",
            "iter 3820: loss 0.8833, time 22.81ms, mfu 15.37%\n",
            "iter 3830: loss 0.9019, time 22.77ms, mfu 15.47%\n",
            "iter 3840: loss 0.8824, time 22.95ms, mfu 15.54%\n",
            "iter 3850: loss 0.8857, time 22.55ms, mfu 15.64%\n",
            "iter 3860: loss 0.8705, time 23.09ms, mfu 15.69%\n",
            "iter 3870: loss 0.8849, time 22.70ms, mfu 15.76%\n",
            "iter 3880: loss 0.8860, time 22.50ms, mfu 15.84%\n",
            "iter 3890: loss 0.8927, time 22.58ms, mfu 15.91%\n",
            "iter 3900: loss 0.8977, time 22.55ms, mfu 15.97%\n",
            "iter 3910: loss 0.8830, time 22.59ms, mfu 16.02%\n",
            "iter 3920: loss 0.8770, time 22.65ms, mfu 16.07%\n",
            "iter 3930: loss 0.8939, time 23.22ms, mfu 16.06%\n",
            "iter 3940: loss 0.8808, time 22.79ms, mfu 16.09%\n",
            "iter 3950: loss 0.8823, time 22.77ms, mfu 16.12%\n",
            "iter 3960: loss 0.8995, time 22.79ms, mfu 16.14%\n",
            "iter 3970: loss 0.8946, time 22.58ms, mfu 16.18%\n",
            "iter 3980: loss 0.8945, time 22.48ms, mfu 16.22%\n",
            "iter 3990: loss 0.8757, time 22.63ms, mfu 16.24%\n",
            "step 4000: train loss 0.7076, val loss 1.6291\n",
            "iter 4000: loss 0.8583, time 3023.54ms, mfu 14.63%\n",
            "iter 4010: loss 0.8834, time 22.66ms, mfu 14.81%\n",
            "iter 4020: loss 0.8926, time 22.99ms, mfu 14.95%\n",
            "iter 4030: loss 0.8748, time 22.66ms, mfu 15.10%\n",
            "iter 4040: loss 0.8788, time 23.71ms, mfu 15.16%\n",
            "iter 4050: loss 0.8702, time 23.19ms, mfu 15.25%\n",
            "iter 4060: loss 0.8609, time 23.07ms, mfu 15.34%\n",
            "iter 4070: loss 0.8654, time 22.73ms, mfu 15.45%\n",
            "iter 4080: loss 0.8798, time 22.68ms, mfu 15.55%\n",
            "iter 4090: loss 0.8499, time 24.66ms, mfu 15.50%\n",
            "iter 4100: loss 0.8952, time 22.87ms, mfu 15.58%\n",
            "iter 4110: loss 0.8650, time 22.87ms, mfu 15.65%\n",
            "iter 4120: loss 0.8821, time 23.18ms, mfu 15.69%\n",
            "iter 4130: loss 0.8526, time 22.76ms, mfu 15.76%\n",
            "iter 4140: loss 0.8726, time 23.27ms, mfu 15.79%\n",
            "iter 4150: loss 0.8668, time 22.78ms, mfu 15.84%\n",
            "iter 4160: loss 0.8525, time 23.13ms, mfu 15.87%\n",
            "iter 4170: loss 0.8628, time 23.18ms, mfu 15.89%\n",
            "iter 4180: loss 0.8651, time 23.69ms, mfu 15.88%\n",
            "iter 4190: loss 0.8662, time 23.18ms, mfu 15.90%\n",
            "iter 4200: loss 0.8539, time 23.12ms, mfu 15.92%\n",
            "iter 4210: loss 0.8694, time 23.02ms, mfu 15.94%\n",
            "iter 4220: loss 0.8646, time 23.17ms, mfu 15.96%\n",
            "iter 4230: loss 0.8756, time 23.06ms, mfu 15.98%\n",
            "iter 4240: loss 0.8622, time 23.40ms, mfu 15.97%\n",
            "step 4250: train loss 0.6767, val loss 1.6533\n",
            "iter 4250: loss 0.8644, time 3039.69ms, mfu 14.39%\n",
            "iter 4260: loss 0.8594, time 22.80ms, mfu 14.58%\n",
            "iter 4270: loss 0.8673, time 23.08ms, mfu 14.74%\n",
            "iter 4280: loss 0.8594, time 22.43ms, mfu 14.93%\n",
            "iter 4290: loss 0.8406, time 22.76ms, mfu 15.07%\n",
            "iter 4300: loss 0.8280, time 22.65ms, mfu 15.21%\n",
            "iter 4310: loss 0.8460, time 24.34ms, mfu 15.22%\n",
            "iter 4320: loss 0.8345, time 22.50ms, mfu 15.35%\n",
            "iter 4330: loss 0.8591, time 22.78ms, mfu 15.45%\n",
            "iter 4340: loss 0.8264, time 22.63ms, mfu 15.55%\n",
            "iter 4350: loss 0.8386, time 22.78ms, mfu 15.64%\n",
            "iter 4360: loss 0.8595, time 22.50ms, mfu 15.73%\n",
            "iter 4370: loss 0.8553, time 22.37ms, mfu 15.82%\n",
            "iter 4380: loss 0.8261, time 22.40ms, mfu 15.90%\n",
            "iter 4390: loss 0.8599, time 22.36ms, mfu 15.98%\n",
            "iter 4400: loss 0.8382, time 22.80ms, mfu 16.02%\n",
            "iter 4410: loss 0.8578, time 22.52ms, mfu 16.07%\n",
            "iter 4420: loss 0.8643, time 22.70ms, mfu 16.10%\n",
            "iter 4430: loss 0.8484, time 22.78ms, mfu 16.13%\n",
            "iter 4440: loss 0.8465, time 22.88ms, mfu 16.14%\n",
            "iter 4450: loss 0.8442, time 22.59ms, mfu 16.18%\n",
            "iter 4460: loss 0.8429, time 22.64ms, mfu 16.21%\n",
            "iter 4470: loss 0.8429, time 22.41ms, mfu 16.25%\n",
            "iter 4480: loss 0.8185, time 22.33ms, mfu 16.29%\n",
            "iter 4490: loss 0.8466, time 22.34ms, mfu 16.33%\n",
            "step 4500: train loss 0.6516, val loss 1.6747\n",
            "iter 4500: loss 0.8528, time 3008.24ms, mfu 14.71%\n",
            "iter 4510: loss 0.8512, time 22.46ms, mfu 14.90%\n",
            "iter 4520: loss 0.8349, time 22.94ms, mfu 15.03%\n",
            "iter 4530: loss 0.8434, time 22.92ms, mfu 15.15%\n",
            "iter 4540: loss 0.8471, time 23.09ms, mfu 15.25%\n",
            "iter 4550: loss 0.8688, time 23.32ms, mfu 15.33%\n",
            "iter 4560: loss 0.8389, time 22.34ms, mfu 15.46%\n",
            "iter 4570: loss 0.8365, time 22.70ms, mfu 15.56%\n",
            "iter 4580: loss 0.8568, time 22.66ms, mfu 15.65%\n",
            "iter 4590: loss 0.8468, time 22.73ms, mfu 15.72%\n",
            "iter 4600: loss 0.8259, time 23.62ms, mfu 15.73%\n",
            "iter 4610: loss 0.8698, time 23.10ms, mfu 15.77%\n",
            "iter 4620: loss 0.8328, time 22.99ms, mfu 15.81%\n",
            "iter 4630: loss 0.8292, time 23.69ms, mfu 15.80%\n",
            "iter 4640: loss 0.8363, time 23.81ms, mfu 15.79%\n",
            "iter 4650: loss 0.8618, time 23.29ms, mfu 15.81%\n",
            "iter 4660: loss 0.8475, time 22.96ms, mfu 15.85%\n",
            "iter 4670: loss 0.8417, time 23.49ms, mfu 15.85%\n",
            "iter 4680: loss 0.8487, time 22.94ms, mfu 15.89%\n",
            "iter 4690: loss 0.8429, time 22.85ms, mfu 15.93%\n",
            "iter 4700: loss 0.8161, time 22.61ms, mfu 15.99%\n",
            "iter 4710: loss 0.7857, time 22.53ms, mfu 16.04%\n",
            "iter 4720: loss 0.8216, time 22.48ms, mfu 16.10%\n",
            "iter 4730: loss 0.8150, time 22.65ms, mfu 16.13%\n",
            "iter 4740: loss 0.8237, time 22.78ms, mfu 16.15%\n",
            "step 4750: train loss 0.6349, val loss 1.6901\n",
            "iter 4750: loss 0.7963, time 3009.79ms, mfu 14.55%\n",
            "iter 4760: loss 0.8142, time 22.52ms, mfu 14.75%\n",
            "iter 4770: loss 0.8015, time 22.47ms, mfu 14.93%\n",
            "iter 4780: loss 0.8118, time 22.92ms, mfu 15.07%\n",
            "iter 4790: loss 0.8419, time 22.92ms, mfu 15.18%\n",
            "iter 4800: loss 0.8137, time 22.45ms, mfu 15.33%\n",
            "iter 4810: loss 0.8429, time 22.63ms, mfu 15.44%\n",
            "iter 4820: loss 0.8252, time 22.77ms, mfu 15.53%\n",
            "iter 4830: loss 0.8216, time 23.11ms, mfu 15.59%\n",
            "iter 4840: loss 0.8379, time 22.31ms, mfu 15.70%\n",
            "iter 4850: loss 0.8154, time 22.77ms, mfu 15.77%\n",
            "iter 4860: loss 0.8176, time 23.12ms, mfu 15.80%\n",
            "iter 4870: loss 0.8070, time 22.82ms, mfu 15.86%\n",
            "iter 4880: loss 0.8327, time 25.25ms, mfu 15.75%\n",
            "iter 4890: loss 0.8079, time 23.14ms, mfu 15.78%\n",
            "iter 4900: loss 0.8057, time 22.66ms, mfu 15.85%\n",
            "iter 4910: loss 0.8297, time 22.86ms, mfu 15.89%\n",
            "iter 4920: loss 0.8221, time 22.60ms, mfu 15.95%\n",
            "iter 4930: loss 0.8135, time 22.41ms, mfu 16.02%\n",
            "iter 4940: loss 0.8022, time 22.92ms, mfu 16.04%\n",
            "iter 4950: loss 0.8284, time 22.78ms, mfu 16.08%\n",
            "iter 4960: loss 0.8254, time 22.62ms, mfu 16.12%\n",
            "iter 4970: loss 0.7870, time 23.31ms, mfu 16.10%\n",
            "iter 4980: loss 0.7907, time 23.05ms, mfu 16.11%\n",
            "iter 4990: loss 0.8267, time 22.34ms, mfu 16.17%\n",
            "step 5000: train loss 0.6223, val loss 1.7001\n",
            "iter 5000: loss 0.8268, time 3020.38ms, mfu 14.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/karpathy/nanoGPT/blob/master/sample.py\n",
        "\n",
        "\"\"\"\n",
        "Sample from a trained model\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "# from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out-shakespeare-char' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "elif init_from.startswith('gpt2'):\n",
        "    # init from a given GPT-2 model\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
        "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    # ok let's assume gpt-2 encodings by default\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZoa0pXCnAz",
        "outputId": "a37b6ee0-ff8c-4ff9-a9ce-b7ed93f483c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And cowards it be strawn as a way to take him so.\n",
            "\n",
            "ISABELLA:\n",
            "Not a purpose to be too that enfray'd him,\n",
            "Doth no other to the town, I cannot live;\n",
            "No, even we send in the sea or virtue,\n",
            "Which now to the senal link of his humour.\n",
            "\n",
            "ANGELO:\n",
            "I am sure, young Baptista doth live your highness than\n",
            "A man of your prince to the proff high him:\n",
            "And your highness prayers to the sorrow you must not know on,\n",
            "Your brother Somerset, Lucio, we did withal\n",
            "With a heart of the presentency,--to change me wi\n",
            "---------------\n",
            "\n",
            "Men pardon, I will not so gracement a child,\n",
            "As you shall never and show the head of your own.\n",
            "\n",
            "JULIET:\n",
            "I'll stay you to the case of the journey,\n",
            "If ever your voices are best.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Here comes me not, sir, come, sir; she shall be dead.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I will not be most my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I have no very complexed him I swear him and honest doubt.\n",
            "\n",
            "LUCIO:\n",
            "Here's the noble mountain of your place. What's her?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "A goodly good direct change, would have me you well.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "Messenger.\n",
            "\n",
            "Second Servingman:\n",
            "Sir, what then? how can you a bawd?\n",
            "\n",
            "CORIOLANUS:\n",
            "Have not seen me this fellow?\n",
            "\n",
            "MENENIUS:\n",
            "This is thine own days\n",
            "a borrow's daughterIng.\n",
            "\n",
            "CORIOLANUS:\n",
            "A lord with you.\n",
            "\n",
            "CORIOLANUS:\n",
            "O thousand in this man to shall enter\n",
            "In all descent. But He was too lately of grace\n",
            "To the other and frown through his house about your in strength\n",
            "At your honour on your thief.\n",
            "\n",
            "SICINIUS:\n",
            "Could the state that he shall be peril'd\n",
            "Where may be ree your general: as you are so are gold,\n",
            "Tha\n",
            "---------------\n",
            "\n",
            "The soldiers will know the shore, as I have done as I say,\n",
            "Could prove of when I do right to revolt my lips.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Believe me, the matter of her eyes,\n",
            "That I was that becomes him in our poor.\n",
            "\n",
            "ROMEO:\n",
            "I have been not, sir, but lord: for that\n",
            "I will give Hermione in the bloody hate two of the noble last.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I will walk a punish of his master, of the same.\n",
            "\n",
            "ROMEO:\n",
            "How mista'en your names, Juliet, we cannot see him with it.\n",
            "\n",
            "JULIET:\n",
            "Dispatch, and noble joiness hence; and to \n",
            "---------------\n",
            "\n",
            "Be ever breath, to think the crown still in the rack,\n",
            "How falseholding, bid the enemy?\n",
            "\n",
            "GLOUCESTER:\n",
            "For what pardon thee love? I will stay them for there.\n",
            "\n",
            "KING HENRY VI:\n",
            "Lords, my good lords, my lord, thy cousin's joy\n",
            "Will find thee thee to my loyally cousin,\n",
            "And more than with the morning of thee.\n",
            "\n",
            "YORK:\n",
            "Why, will not I would not have no less slight?\n",
            "\n",
            "BUCKINGHAM:\n",
            "They have done to me but down the other's eye,\n",
            "And that move be my stamp without the course\n",
            "Of my knight father; but to quench with \n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "Having you, noble noble gods!\n",
            "\n",
            "SICINIUS:\n",
            "No, sir.\n",
            "\n",
            "BRUTUS:\n",
            "Coriolanus, come, come, come on, for you\n",
            "By all the portern.\n",
            "\n",
            "MENENIUS:\n",
            "You are word, take something of blame. Come, see\n",
            "The coldier of this the people.\n",
            "\n",
            "First Senator:\n",
            "Go three to the music and dutify the prison,\n",
            "So doth an unhappy hour and Baptista,\n",
            "Which you cannot dismiss the aid of our eye;\n",
            "For, be your thing contraction will appear,\n",
            "You may have made your words, that the searching have been\n",
            "To be consul. I will not stay \n",
            "---------------\n",
            "\n",
            "Shepherd: but he is so, it was my father,\n",
            "I know no sheep-seeing in this all, the whole\n",
            "Have washed your blood, who it brought\n",
            "That his presaging and therefore I have my other\n",
            "Fellow my lord. Hermioner, and for my lord,\n",
            "What shall not pass of gift me would, and unrance to my shame,\n",
            "And then within the fields of you and my fair loving.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Torture them to her speak. You will acquainted\n",
            "A word with him and enemy.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I would your princexs blows to my love but life,\n",
            "He is \n",
            "---------------\n",
            "\n",
            "I'll play them on the foe, or thou not as what do there,\n",
            "Have I an arm'd to order that thou love thee was and lay,\n",
            "As thou wilt written, and that wilt not wash her brother;\n",
            "And yet to all this she wakes from the high hour\n",
            "Of a servant great death of their ears and stare tears.\n",
            "Therefore, I remember the seat, the should we go come.\n",
            "\n",
            "Nurse:\n",
            "Hence, tell me thy hand, that hath some amended with thee?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "I pray thee, set thou thee of that charm.\n",
            "No, no, she wakes me for my sweet cold.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The realm and his pairs\n",
            "In his person: therefore is come to his bride\n",
            "Him to't and see sometime half as he shall scarce the sea,\n",
            "Till the cleavelting of his face may him:\n",
            "The heavens mark him to his own princely,\n",
            "Of whom I wrong thee and sure of many with the bosom,\n",
            "With any smoothing beasts to one and that thou art.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Then how she was the duke sitter to truth\n",
            "And the here from the shoulder so relate,\n",
            "And her is it was a thing with the high ruthless feast.\n",
            "\n",
            "FRIAR LAURENC\n",
            "---------------\n",
            "\n",
            "He hath not settledged\n",
            "To himself and spotted forth him: 'Seators best for money!\n",
            "Meantimes are part of battless at some house\n",
            "Than the thing. Should have you? for man fast,\n",
            "For he had virtues already shallow the right,\n",
            "And sentenced, stir, like a present in a little need\n",
            "That the way for beauty as the stroke as plant.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Have you no more to that your sacred?\n",
            "\n",
            "ANGELO:\n",
            "When he comes me? what thinks I speak it in dead?\n",
            "\n",
            "ISABELLA:\n",
            "His nurse is for this that, he comes more a wooer thre\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ykWLut2PW24Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
