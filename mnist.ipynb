{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d2ad6b",
   "metadata": {},
   "source": [
    "# MNIST classification with FC/CNN using NumPy/PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b09b2c",
   "metadata": {},
   "source": [
    "## FC-NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61471687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.getcwd() \n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "for file_name in key_file.values():\n",
    "    \n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    \n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "\n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "print(\"Creating pickle file ...\")\n",
    "with open(save_file, 'wb') as f:\n",
    "    pickle.dump(dataset, f, -1)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859d75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['train_img', 'train_label', 'test_img', 'test_label'])\n",
      "60000 10000 60000 10000\n"
     ]
    }
   ],
   "source": [
    "with open(save_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(type(dataset))\n",
    "print(dataset.keys())\n",
    "print(len(dataset['train_img']), len(dataset['test_img']), len(dataset['train_label']), len(dataset['test_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5593aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 784)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(2, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def _into_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "def load_mnist(flatten=True):\n",
    "\n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    # normalize the pixel values between 0 and 1\n",
    "    for key in ('train_img', 'test_img'):\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "        dataset[key] /= 255.0\n",
    "    \n",
    "    # convert the label into one-hot label\n",
    "    dataset['train_label'] = _into_one_hot_label(dataset['train_label'])\n",
    "    dataset['test_label'] = _into_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    # flatten\n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True)\n",
    "\n",
    "x_batch = x_train[:2]\n",
    "t_batch = t_train[:2]\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print(t_batch.shape)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d30101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9f5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class FCNumPyMNISTClassifier:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # parameters\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # affine-relu-affine-softmaxCE\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "model = FCNumPyMNISTClassifier(input_size=784, hidden_size=100, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f23eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.024020435594987e-10\n",
      "b1:3.0673714093217034e-09\n",
      "W2:5.279488153366296e-09\n",
      "b2:1.6060616138402484e-07\n"
     ]
    }
   ],
   "source": [
    "grad_numerical = model.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = model.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e932147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0878, 0.0866\n",
      "0.9057, 0.9088\n",
      "0.9253, 0.9269\n",
      "0.9386, 0.9391\n",
      "0.9465, 0.9449\n",
      "0.9558, 0.9524\n",
      "0.9596, 0.9542\n",
      "0.9657, 0.9624\n",
      "0.9696, 0.9645\n",
      "0.9725, 0.9681\n",
      "0.9753, 0.9701\n",
      "0.9771, 0.9697\n",
      "0.9788, 0.9728\n",
      "0.9799, 0.9720\n",
      "0.9820, 0.9732\n",
      "0.9830, 0.9751\n",
      "0.9836, 0.9758\n",
      "0.9849, 0.9750\n",
      "0.9857, 0.9754\n",
      "0.9869, 0.9771\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "iter_per_epoch = x_train.shape[0]//batch_size\n",
    "\n",
    "for i in range(epochs * iter_per_epoch):\n",
    "    \n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    loss = model.loss(x_batch, t_batch)\n",
    "\n",
    "    # grad = model.numerical_gradient(x_batch, t_batch)\n",
    "    grad = model.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        model.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = model.accuracy(x_train, t_train)\n",
    "        test_acc = model.accuracy(x_test, t_test)\n",
    "        print(f'{train_acc:.4f}, {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c744ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n",
      "Prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = random.randint(0, len(dataset['test_img'])-1)\n",
    "X_random = dataset['test_img'][r:r+1]\n",
    "Y_random = dataset['test_label'][r:r+1]\n",
    "\n",
    "print(f\"Label: {Y_random[0]}\")\n",
    "print(f\"Prediction: {np.argmax(softmax(model.predict(X_random)))}\")\n",
    "plt.imshow(X_random.reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c471da2",
   "metadata": {},
   "source": [
    "## CNN-NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342d2fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.getcwd() \n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "for file_name in key_file.values():\n",
    "    \n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    \n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "\n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "print(\"Creating pickle file ...\")\n",
    "with open(save_file, 'wb') as f:\n",
    "    pickle.dump(dataset, f, -1)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace2562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['train_img', 'train_label', 'test_img', 'test_label'])\n",
      "60000 10000 60000 10000\n"
     ]
    }
   ],
   "source": [
    "with open(save_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(type(dataset))\n",
    "print(dataset.keys())\n",
    "print(len(dataset['train_img']), len(dataset['test_img']), len(dataset['train_label']), len(dataset['test_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50307ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 28, 28)\n",
      "[[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
      "(2, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def _into_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "def load_mnist(flatten=True):\n",
    "\n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    # normalize the pixel values between 0 and 1\n",
    "    for key in ('train_img', 'test_img'):\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "        dataset[key] /= 255.0\n",
    "    \n",
    "    # convert the label into one-hot label\n",
    "    dataset['train_label'] = _into_one_hot_label(dataset['train_label'])\n",
    "    dataset['test_label'] = _into_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    # flatten\n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_batch = x_train[:2]\n",
    "t_batch = t_train[:2]\n",
    "\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print(t_batch.shape)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3282987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32dd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class CNNNumPyMNISTClassifier:\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim=(1, 28, 28), \n",
    "                 conv1_out_channel=32, \n",
    "                 conv2_out_channel=64,\n",
    "                 output_size=10, \n",
    "                 filter_size = 3,\n",
    "                 filter_pad = 1,\n",
    "                 filter_stride = 1,\n",
    "                 weight_init_std=0.01):\n",
    "        \n",
    "        # affine_in_feature을 구하기 위한 과정\n",
    "        input_width = input_dim[1]\n",
    "        # 28\n",
    "        conv1_out_width = int((input_width - filter_size + 2*filter_pad) / filter_stride + 1)\n",
    "        # 28\n",
    "        pool1_out_width = conv1_out_width//2\n",
    "        # 14\n",
    "        conv2_out_width = int((pool1_out_width - filter_size + 2*filter_pad) / filter_stride + 1)\n",
    "        # 14\n",
    "        pool2_out_width = conv2_out_width//2\n",
    "        # 7\n",
    "        affine_in_feature = int(conv2_out_channel * pool2_out_width * pool2_out_width)\n",
    "        # 3136\n",
    "\n",
    "        # parameters\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(conv1_out_channel, input_dim[0], filter_size, filter_size) * weight_init_std\n",
    "        self.params['b1'] = np.zeros(conv1_out_channel)\n",
    "        self.params['W2'] = np.random.randn(conv2_out_channel, conv1_out_channel, filter_size, filter_size) * weight_init_std\n",
    "        self.params['b2'] = np.zeros(conv2_out_channel)\n",
    "        self.params['W3'] = np.random.randn(affine_in_feature, output_size) * weight_init_std\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # conv-relu-pool-conv-relu-pool-affine-softmaxCE\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], stride=filter_stride, pad=filter_pad)\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Conv2'] = Convolution(self.params['W2'], self.params['b2'], stride=filter_stride, pad=filter_pad)\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])        \n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Conv2'].dW\n",
    "        grads['b2'] = self.layers['Conv2'].db\n",
    "        grads['W3'] = self.layers['Affine'].dW\n",
    "        grads['b3'] = self.layers['Affine'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "model = CNNNumPyMNISTClassifier(input_dim=(1, 28, 28), \n",
    "                                conv1_out_channel=32, \n",
    "                                conv2_out_channel=64,\n",
    "                                output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d31e6490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.5971456179127626e-05\n",
      "b1:0.0029062804043156303\n",
      "W2:6.006672755723734e-07\n",
      "b2:0.008121639318308388\n",
      "W3:1.1912013437527102e-10\n",
      "b3:1.5994033273486474e-07\n"
     ]
    }
   ],
   "source": [
    "grad_numerical = model.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = model.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019a728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1112, 0.1075\n",
      "0.1124, 0.1135\n",
      "0.7983, 0.8090\n",
      "0.8894, 0.8964\n",
      "0.9071, 0.9151\n",
      "0.9177, 0.9259\n",
      "0.9380, 0.9419\n",
      "0.9409, 0.9437\n",
      "0.9527, 0.9556\n",
      "0.9604, 0.9643\n",
      "0.9634, 0.9629\n",
      "0.9659, 0.9688\n",
      "0.9681, 0.9674\n",
      "0.9704, 0.9707\n",
      "0.9704, 0.9707\n",
      "0.9753, 0.9747\n",
      "0.9723, 0.9732\n",
      "0.9772, 0.9756\n",
      "0.9781, 0.9763\n",
      "0.9786, 0.9775\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "iter_per_epoch = x_train.shape[0]//batch_size\n",
    "\n",
    "for i in range(epochs * iter_per_epoch):\n",
    "    \n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    loss = model.loss(x_batch, t_batch)\n",
    "\n",
    "    # grad = model.numerical_gradient(x_batch, t_batch)\n",
    "    grad = model.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        model.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = model.accuracy(x_train, t_train)\n",
    "        test_acc = model.accuracy(x_test, t_test)\n",
    "        print(f'{train_acc:.4f}, {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0563278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "Prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r = random.randint(0, len(dataset['test_img'])-1)\n",
    "X_random = dataset['test_img'][r:r+1].reshape(28, 28)\n",
    "Y_random = dataset['test_label'][r:r+1]\n",
    "\n",
    "print(f\"Label: {Y_random[0]}\")\n",
    "print(f\"Prediction: {np.argmax(softmax(model.predict(np.expand_dims(np.expand_dims(X_random, 0), 0))))}\")\n",
    "plt.imshow(X_random, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389e23a",
   "metadata": {},
   "source": [
    "## FC-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c8dd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f98bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a42423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if device=='cuda':\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7c691fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root=\".\",\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\".\",\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2abb126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataloader = DataLoader(dataset=mnist_train,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=True,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a013c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCPyTorchMNISTClassifier(\n",
      "  (layer1): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FCPyTorchMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = FCPyTorchMNISTClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3cec663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 100, 100]          78,500\n",
      "              ReLU-2             [-1, 100, 100]               0\n",
      "            Linear-3              [-1, 100, 10]           1,010\n",
      "================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.30\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 0.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(100, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae116840",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb9fcb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 Cost: 0.523085\n",
      "Epoch:    1 Cost: 0.273585\n",
      "Epoch:    2 Cost: 0.220413\n",
      "Epoch:    3 Cost: 0.184676\n",
      "Epoch:    4 Cost: 0.158337\n",
      "Epoch:    5 Cost: 0.137529\n",
      "Epoch:    6 Cost: 0.121570\n",
      "Epoch:    7 Cost: 0.108772\n",
      "Epoch:    8 Cost: 0.098338\n",
      "Epoch:    9 Cost: 0.089907\n",
      "Epoch:   10 Cost: 0.082451\n",
      "Epoch:   11 Cost: 0.076391\n",
      "Epoch:   12 Cost: 0.071078\n",
      "Epoch:   13 Cost: 0.065836\n",
      "Epoch:   14 Cost: 0.061706\n",
      "Epoch:   15 Cost: 0.057744\n",
      "Epoch:   16 Cost: 0.054726\n",
      "Epoch:   17 Cost: 0.051594\n",
      "Epoch:   18 Cost: 0.048461\n",
      "Epoch:   19 Cost: 0.045837\n",
      "Epoch:   20 Cost: 0.043598\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs+1):\n",
    "    \n",
    "    avg_cost = 0\n",
    "    total_batch = len(dataloader)\n",
    "    \n",
    "    for X, Y in dataloader:\n",
    "        X = X.view(-1, 28*28).to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        cost = criterion(pred, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost/total_batch\n",
    "        \n",
    "    print(f'Epoch: {epoch:4d} Cost: {avg_cost:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12e902f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9739\n",
      "Label: 2\n",
      "Prediction: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/xcda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:69: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/xcda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:59: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(f\"Accuracy: {accuracy.item():.4f}\")\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_random = mnist_test.test_data[r:r+1].view(-1, 28*28).float().to(device)\n",
    "    Y_random = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print(f\"Label: {Y_random.item()}\")\n",
    "    single_prediction = model(X_random)\n",
    "    print(f\"Prediction: {torch.argmax(model(X_random), 1).item()}\")\n",
    "    plt.imshow(mnist_test.test_data[r:r+1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa99c8",
   "metadata": {},
   "source": [
    "## CNN-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e29ea1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "002b1423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4a422d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if device=='cuda':\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2b285e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(root=\".\",\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\".\",\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9be538a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataloader = DataLoader(dataset=mnist_train,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=True,\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "844909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNPyTorchMNISTClassifier(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNPyTorchMNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(7*7*64, 10)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = CNNPyTorchMNISTClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b4f4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [100, 32, 28, 28]             320\n",
      "              ReLU-2          [100, 32, 28, 28]               0\n",
      "         MaxPool2d-3          [100, 32, 14, 14]               0\n",
      "            Conv2d-4          [100, 64, 14, 14]          18,496\n",
      "              ReLU-5          [100, 64, 14, 14]               0\n",
      "         MaxPool2d-6            [100, 64, 7, 7]               0\n",
      "            Linear-7                  [100, 10]          31,370\n",
      "================================================================\n",
      "Total params: 50,186\n",
      "Trainable params: 50,186\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.30\n",
      "Forward/backward pass size (MB): 64.61\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 65.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1, 28, 28), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80763734",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79228a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 Cost: 0.950457\n",
      "Epoch:    1 Cost: 0.326900\n",
      "Epoch:    2 Cost: 0.254949\n",
      "Epoch:    3 Cost: 0.206060\n",
      "Epoch:    4 Cost: 0.170603\n",
      "Epoch:    5 Cost: 0.144487\n",
      "Epoch:    6 Cost: 0.125992\n",
      "Epoch:    7 Cost: 0.112185\n",
      "Epoch:    8 Cost: 0.101375\n",
      "Epoch:    9 Cost: 0.093899\n",
      "Epoch:   10 Cost: 0.087399\n",
      "Epoch:   11 Cost: 0.081997\n",
      "Epoch:   12 Cost: 0.077801\n",
      "Epoch:   13 Cost: 0.074023\n",
      "Epoch:   14 Cost: 0.070530\n",
      "Epoch:   15 Cost: 0.067960\n",
      "Epoch:   16 Cost: 0.065175\n",
      "Epoch:   17 Cost: 0.062499\n",
      "Epoch:   18 Cost: 0.060749\n",
      "Epoch:   19 Cost: 0.058910\n",
      "Epoch:   20 Cost: 0.057172\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs+1):\n",
    "    \n",
    "    avg_cost = 0\n",
    "    total_batch = len(dataloader)\n",
    "\n",
    "    for X, Y in dataloader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        cost = criterion(pred, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost/total_batch\n",
    "        \n",
    "    print(f'Epoch: {epoch:4d} Cost: {avg_cost:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e054ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9820\n",
      "Label: 2\n",
      "Prediction: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1)==Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(f\"Accuracy: {accuracy.item():.4f}\")\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_random = torch.unsqueeze(mnist_test.test_data[r:r+1].float().to(device), 0)\n",
    "    Y_random = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print(f\"Label: {Y_random.item()}\")\n",
    "    single_prediction = model(X_random)\n",
    "    print(f\"Prediction: {torch.argmax(model(X_random), 1).item()}\")\n",
    "    plt.imshow(mnist_test.test_data[r:r+1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf9253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
