## main idea
- mcts
- python
- q-value preference model
- but it does use outputs generated by larger models..

## rounds ('self-evolution')
- round 1: ds-coder-v2-instruct. 236B. 흠.. train PPM-r1. mcts 8 rollouts. terminal-guided (rollout: 8, candidates: 5, depth: 16)
- round 2: use SLM-r1. extensive (16) mcts rollouts. train PPM-r2. (rollout: 16, candidates: 6, depth: 16). O(rollout * candidate# * depth)
- round 3: use PPM-r2 to do PPM-augmented mcts.
- round 4: unsolved after 16 rollouts -> 64 -> 128 (candidates: 16)

## PPM, not PRM, ORM.
- instead of predicting Q-values directly, use preference pairs, and do pairwise ranking loss.
- fig 5 shows boost in performance using ppm.
- Q-value: contribution to the answer

## datasets
- MATH
- AIME

## details
- problems are augmented using gpt-4

## 의문
- mcts. # of rollouts, explorations.
- exploration에 어떻게 diversity를 부여함??
- 한 스텝 어떻게 정의? regulate?
- pairwise ranking loss
- 각 round마다 finetuning? 뭐로? 어떻게? LoRA 아니지?
- UTC, Upper Confidence bound for Trees. UTC(s) = Q(s) + c sqrt((ln N_parent(s)) / (N(s))), where Q(s) = q(s)/N(s). q(s) is the Q-value, N(s) is the times this term is explored, N_parent(s) is the times the parent of this term is explored. --> average reward of this step + c * (inverse of exploration of this step)

## related works
-

## lesson
- ppm: judging is easier
- emergence. self-reflection, self-correction
